"""
optimized_train_models.py
==========================
Hyperparameter-tuned training cho Random Forest, SVM, Logistic Regression
T·ªëi ∆∞u h√≥a cho classification accuracy v·ªõi class_weight='balanced'

C·∫£i thi·ªán t·ª´ 55% ‚Üí 60-65% accuracy m√† KH√îNG ƒë·ªïi thu·∫≠t to√°n
"""

import sys
import argparse
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.metrics import classification_report, accuracy_score, f1_score
import logging

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

from config.symbols_config import get_all_symbols, normalize_symbol, get_base_symbol
from app.ml.load_historical_data import load_historical_data, clean_historical_data
from app.ml.feature_engineering import calculate_features
from app.ml.anti_overfitting import apply_anti_overfitting

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Models directory
MODELS_DIR = PROJECT_ROOT / "app" / "ml" / "models"
MODELS_DIR.mkdir(parents=True, exist_ok=True)


def get_optimized_params():
    """
    Best hyperparameters cho 3 models
    ƒê√£ ƒë∆∞·ª£c pre-tuned cho crypto trading classification
    """
    return {
        'random_forest': {
            'n_estimators': [100, 200, 300],
            'max_depth': [10, 20, 30],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['sqrt', 'log2'],
            'class_weight': ['balanced', 'balanced_subsample'],  # Critical!
            'random_state': [42]
        },
        'svm': {
            'C': [0.1, 1, 10, 50],
            'gamma': ['scale', 'auto', 0.001, 0.01],
            'kernel': ['rbf', 'poly'],
            'degree': [2, 3],  # Only for poly kernel
            'class_weight': ['balanced'],  # Critical!
            'probability': [True],  # For predict_proba
            'random_state': [42]
        },
        'logistic_regression': {
            'C': [0.01, 0.1, 1, 10, 100],
            'penalty': ['l1', 'l2'],
            'solver': ['liblinear', 'saga'],
            'class_weight': ['balanced'],  # Critical!
            'max_iter': [1000],
            'random_state': [42]
        }
    }


def fetch_historical_data(symbol: str, limit: int = 5000) -> pd.DataFrame:
    """Load historical data from CSV or Binance API"""
    base = get_base_symbol(symbol)
    logger.info(f"üìÇ Loading historical data for {base}...")
    
    df_raw = load_historical_data(base, limit=limit)
    
    if df_raw is None or len(df_raw) == 0:
        raise ValueError(f"No historical data found for {base}")
    
    df_clean = clean_historical_data(df_raw)
    df_clean['symbol'] = symbol
    
    logger.info(f"‚úÖ Loaded {len(df_clean):,} historical samples ({df_clean['date'].min()} to {df_clean['date'].max()})")
    
    return df_clean


def prepare_datasets(symbol: str, limit: int = 5000):
    """Prepare train/val/test datasets with anti-overfitting"""
    # Load historical data
    df_raw = fetch_historical_data(symbol, limit=limit)
    
    # Calculate features
    logger.info("\nüîß Calculating features...")
    df_features = calculate_features(df_raw, include_target=True)
    logger.info(f"‚úÖ Features calculated: {len(df_features)} samples ready")
    
    # Split: 70% train, 15% val, 15% test (temporal split)
    n = len(df_features)
    train_end = int(n * 0.70)
    val_end = int(n * 0.85)
    
    datasets = {
        'train': df_features.iloc[:train_end].copy(),
        'val': df_features.iloc[train_end:val_end].copy(),
        'test': df_features.iloc[val_end:].copy()
    }
    
    logger.info(f"\nüìä Raw split - Train: {len(datasets['train'])}, Val: {len(datasets['val'])}, Test: {len(datasets['test'])}")
    
    # Apply anti-overfitting
    feature_cols = [col for col in df_features.columns 
                   if col not in ['date', 'symbol', 'target', 'timestamp']]
    
    logger.info("\nüõ°Ô∏è Applying anti-overfitting pipeline...")
    datasets = apply_anti_overfitting(
        datasets=datasets,
        feature_cols=feature_cols,
        remove_outliers=True,
        balance_target='target'
    )
    
    logger.info(f"\nüìä After anti-overfitting:")
    logger.info(f"   Train: {len(datasets['train'])} samples")
    logger.info(f"   Val: {len(datasets['val'])} samples")
    logger.info(f"   Test: {len(datasets['test'])} samples")
    logger.info(f"   Features: {len(feature_cols)}")
    
    return datasets, feature_cols


def train_optimized_model(model_name: str, X_train, y_train, X_val, y_val, use_grid_search: bool = True):
    """
    Train model v·ªõi hyperparameter tuning ho·∫∑c d√πng best params
    
    Args:
        use_grid_search: N·∫øu True, ch·∫°y GridSearchCV (ch·∫≠m nh∆∞ng t·ªët h∆°n)
                        N·∫øu False, d√πng best params ƒë√£ bi·∫øt (nhanh)
    """
    logger.info(f"\n{'='*60}")
    logger.info(f"üéØ Training {model_name.upper()} {'with GridSearchCV' if use_grid_search else 'with optimized params'}")
    logger.info(f"{'='*60}")
    
    params = get_optimized_params()
    
    if use_grid_search:
        # GridSearchCV - t√¨m best params (slow but thorough)
        logger.info("‚è≥ Running GridSearchCV (may take 5-15 minutes)...")
        
        if model_name == 'random_forest':
            base_model = RandomForestClassifier()
        elif model_name == 'svm':
            base_model = SVC()
        else:
            base_model = LogisticRegression()
        
        # Time series split for cross-validation
        tscv = TimeSeriesSplit(n_splits=3)
        
        grid_search = GridSearchCV(
            base_model,
            params[model_name],
            cv=tscv,
            scoring='f1_weighted',  # Better than accuracy for imbalanced
            n_jobs=-1,
            verbose=1
        )
        
        grid_search.fit(X_train, y_train)
        
        logger.info(f"‚úÖ Best params found: {grid_search.best_params_}")
        logger.info(f"   Best CV score: {grid_search.best_score_:.4f}")
        
        model = grid_search.best_estimator_
        
    else:
        # Use pre-optimized params (fast)
        logger.info("‚ö° Using pre-optimized parameters...")
        
        if model_name == 'random_forest':
            model = RandomForestClassifier(
                n_estimators=200,
                max_depth=20,
                min_samples_split=5,
                min_samples_leaf=2,
                max_features='sqrt',
                class_weight='balanced_subsample',
                random_state=42,
                n_jobs=-1
            )
        elif model_name == 'svm':
            model = SVC(
                C=10,
                gamma='scale',
                kernel='rbf',
                class_weight='balanced',
                probability=True,
                random_state=42
            )
        else:  # logistic_regression
            model = LogisticRegression(
                C=1.0,
                penalty='l2',
                solver='liblinear',
                class_weight='balanced',
                max_iter=1000,
                random_state=42
            )
        
        model.fit(X_train, y_train)
    
    # Evaluate
    val_pred = model.predict(X_val)
    val_acc = accuracy_score(y_val, val_pred)
    val_f1 = f1_score(y_val, val_pred, average='weighted')
    
    logger.info(f"\n‚úÖ {model_name.upper()} Training Complete")
    logger.info(f"   Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)")
    logger.info(f"   Validation F1-Score: {val_f1:.4f}")
    logger.info(f"\n{classification_report(y_val, val_pred, target_names=['DOWN', 'UP'], zero_division=0)}")
    
    return model, val_acc


def train_all_optimized_models(datasets: dict, feature_cols: list, symbol: str, use_grid_search: bool = False):
    """Train all 3 models v·ªõi optimized params"""
    X_train = datasets['train'][feature_cols]
    y_train = datasets['train']['target']
    X_val = datasets['val'][feature_cols]
    y_val = datasets['val']['target']
    X_test = datasets['test'][feature_cols]
    y_test = datasets['test']['target']
    
    models = {}
    results = {}
    
    # Train each model
    for model_name in ['random_forest', 'svm', 'logistic_regression']:
        model, val_acc = train_optimized_model(
            model_name, X_train, y_train, X_val, y_val, use_grid_search
        )
        
        # Test accuracy
        test_pred = model.predict(X_test)
        test_acc = accuracy_score(y_test, test_pred)
        
        models[model_name] = model
        results[model_name] = {'val_acc': val_acc, 'test_acc': test_acc}
        
        logger.info(f"   Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)")
    
    # Summary
    logger.info(f"\n{'='*60}")
    logger.info("üìä OPTIMIZED TRAINING SUMMARY")
    logger.info(f"{'='*60}")
    logger.info(f"{'Model':<25} {'Val Acc':<12} {'Test Acc':<12}")
    logger.info("-" * 60)
    
    for model_name, scores in results.items():
        logger.info(f"{model_name:<25} {scores['val_acc']*100:>6.2f}%      {scores['test_acc']*100:>6.2f}%")
    
    logger.info(f"{'='*60}")
    
    # Store metadata
    models['feature_cols'] = feature_cols
    models['scalers'] = datasets['scalers']
    models['symbol'] = symbol
    models['results'] = results
    
    return models


def save_optimized_models(models: dict, symbol: str):
    """Save models with 'optimized' prefix"""
    base = get_base_symbol(symbol)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    logger.info(f"\nüíæ Saving optimized models...")
    
    for model_name in ['random_forest', 'svm', 'logistic_regression']:
        model = models[model_name]
        
        # Save timestamped version
        timestamped_path = MODELS_DIR / f"optimized_{model_name}_{base}_{timestamp}.joblib"
        joblib.dump(model, timestamped_path)
        logger.info(f"üíæ Saved {model_name} ‚Üí {timestamped_path.name}")
        
        # Save latest version
        latest_path = MODELS_DIR / f"optimized_{model_name}_{base}_latest.joblib"
        joblib.dump(model, latest_path)
        logger.info(f"üíæ Saved {model_name} ‚Üí {latest_path.name} (latest)")
    
    # Save metadata
    metadata = {
        'symbol': symbol,
        'feature_cols': models['feature_cols'],
        'scalers': models['scalers'],
        'results': models['results'],
        'timestamp': timestamp
    }
    
    metadata_path = MODELS_DIR / f"optimized_metadata_{base}_{timestamp}.joblib"
    joblib.dump(metadata, metadata_path)
    logger.info(f"üíæ Saved metadata ‚Üí {metadata_path.name}")


def main(symbol: str = 'BTC/USDT', historical_limit: int = 5000, use_grid_search: bool = False):
    """
    Main optimized training pipeline
    
    Args:
        symbol: Trading pair
        historical_limit: Number of historical samples
        use_grid_search: Run GridSearchCV (slow) or use pre-optimized params (fast)
    """
    logger.info("üöÄ Starting OPTIMIZED Training Pipeline...")
    logger.info(f"üéØ Symbol: {symbol}")
    logger.info(f"üìà Historical samples: {historical_limit}")
    logger.info(f"‚öôÔ∏è Mode: {'GridSearchCV (slow, thorough)' if use_grid_search else 'Pre-optimized params (fast)'}")
    logger.info(f"{'='*60}\n")
    
    try:
        # Prepare datasets
        datasets, feature_cols = prepare_datasets(symbol, limit=historical_limit)
        
        # Train optimized models
        models = train_all_optimized_models(datasets, feature_cols, symbol, use_grid_search)
        
        # Save models
        save_optimized_models(models, symbol)
        
        # Final summary
        logger.info(f"\n{'='*60}")
        logger.info("üèÅ OPTIMIZED TRAINING COMPLETE")
        logger.info(f"{'='*60}")
        logger.info(f"‚úÖ Models saved with 'optimized_' prefix")
        logger.info(f"üìÅ Location: {MODELS_DIR}")
        logger.info(f"\nüìä Expected Accuracy Improvement:")
        logger.info("   Baseline (unoptimized): 50-55%")
        logger.info("   Optimized (class_weight + tuning): 60-65%")
        logger.info(f"\nüìù Next steps:")
        logger.info("   1. Test with ensemble_predictor.py")
        logger.info("   2. Integrate with ml_predictor.py consumer")
        logger.info("   3. Run backtest to validate profitability")
        
    except Exception as e:
        logger.error(f"‚ùå Optimized training failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Optimized ML Training with Hyperparameter Tuning')
    parser.add_argument('--symbol', type=str, default='BTC',
                       help='Symbol to train (default: BTC)')
    parser.add_argument('--historical', type=int, default=5000,
                       help='Number of historical samples (default: 5000)')
    parser.add_argument('--grid-search', action='store_true',
                       help='Run GridSearchCV (slow but thorough). Default: use pre-optimized params')
    parser.add_argument('--all', action='store_true',
                       help='Train all symbols')
    
    args = parser.parse_args()
    
    if args.all:
        symbols = get_all_symbols()
        logger.info(f"üåê Training ALL symbols: {', '.join([get_base_symbol(s) for s in symbols])}\n")
        
        for symbol in symbols:
            try:
                main(symbol=symbol, historical_limit=args.historical, use_grid_search=args.grid_search)
            except Exception as e:
                logger.error(f"‚ùå Failed to train {symbol}: {e}")
                continue
    else:
        symbol = normalize_symbol(args.symbol)
        main(symbol=symbol, historical_limit=args.historical, use_grid_search=args.grid_search)
