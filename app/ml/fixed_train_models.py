# fixed_train_models.py  
# Training script S·ª¨A L·ªñI data leakage v√† overfitting

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
import joblib
import json
import os
import sys
from datetime import datetime
from typing import Dict, Any, Tuple
import warnings
warnings.filterwarnings('ignore')

sys.path.append(os.path.dirname(__file__))
from fixed_data_prep import prepare_ml_datasets_fixed, project_root_path

class FixedMLTrainer:
    """
    ML Trainer v·ªõi c√°c s·ª≠a l·ªói:
    1. Kh√¥ng c√≥ data leakage
    2. Regularization ƒë·ªÉ tr√°nh overfitting
    3. Proper cross-validation
    4. Realistic evaluation metrics
    """
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.results = {}
        
    def train_regression_models(self, datasets: Dict) -> Dict:
        """
        Train regression models v·ªõi regularization
        """
        print("\nüîß TRAINING REGRESSION MODELS (FIXED)")
        print("="*50)
        
        X_train = datasets['X_train']
        X_val = datasets['X_val'] 
        X_test = datasets['X_test']
        
        # Normalize features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)
        
        self.scalers['regression'] = scaler
        
        regression_results = {}
        
        # Target: Price prediction (1 hour ahead)
        print("\n1Ô∏è‚É£ Price Prediction (1 hour)")
        y_train = datasets['y_train']['price_1h']
        y_val = datasets['y_val']['price_1h']
        y_test = datasets['y_test']['price_1h']
        
        # Multiple models with regularization
        price_models = {
            'linear_regression': LinearRegression(),
            'ridge_regression': Ridge(alpha=1.0),  # L2 regularization
            'lasso_regression': Lasso(alpha=0.1),  # L1 regularization  
            'random_forest': RandomForestRegressor(
                n_estimators=50,  # Reduced to prevent overfitting
                max_depth=10,     # Limit depth
                min_samples_split=10,  # Require more samples to split
                min_samples_leaf=5,    # Require more samples in leaf
                random_state=42
            )
        }
        
        price_results = {}
        for name, model in price_models.items():
            print(f"\n  üî∏ Training {name}...")
            
            # Train model
            model.fit(X_train_scaled, y_train)
            
            # Predictions
            train_pred = model.predict(X_train_scaled)
            val_pred = model.predict(X_val_scaled)
            test_pred = model.predict(X_test_scaled)
            
            # Metrics
            metrics = {
                'train': {
                    'mae': mean_absolute_error(y_train, train_pred),
                    'rmse': np.sqrt(mean_squared_error(y_train, train_pred)),
                    'r2': r2_score(y_train, train_pred)
                },
                'val': {
                    'mae': mean_absolute_error(y_val, val_pred),
                    'rmse': np.sqrt(mean_squared_error(y_val, val_pred)),
                    'r2': r2_score(y_val, val_pred)
                },
                'test': {
                    'mae': mean_absolute_error(y_test, test_pred),
                    'rmse': np.sqrt(mean_squared_error(y_test, test_pred)),
                    'r2': r2_score(y_test, test_pred)
                }
            }
            
            # Check for overfitting
            train_r2 = metrics['train']['r2']
            val_r2 = metrics['val']['r2']
            overfitting_score = abs(train_r2 - val_r2)
            
            print(f"    üìä Results:")
            print(f"      Train R¬≤: {train_r2:.4f}")
            print(f"      Val R¬≤:   {val_r2:.4f}")
            print(f"      Test R¬≤:  {metrics['test']['r2']:.4f}")
            print(f"      Overfitting: {overfitting_score:.4f} {'‚ö†Ô∏è HIGH' if overfitting_score > 0.1 else '‚úÖ OK'}")
            
            price_results[name] = {
                'model': model,
                'metrics': metrics,
                'overfitting_score': overfitting_score
            }
        
        regression_results['price_1h'] = price_results
        
        # Target: Price change prediction (3 hours)
        print("\n2Ô∏è‚É£ Price Change Prediction (3 hours)")
        y_train_change = datasets['y_train']['change_3h']
        y_val_change = datasets['y_val']['change_3h']
        y_test_change = datasets['y_test']['change_3h']
        
        change_models = {
            'ridge_regression': Ridge(alpha=10.0),  # Higher regularization for change prediction
            'random_forest': RandomForestRegressor(
                n_estimators=30,
                max_depth=8,
                min_samples_split=15,
                min_samples_leaf=8,
                random_state=42
            )
        }
        
        change_results = {}
        for name, model in change_models.items():
            print(f"\n  üî∏ Training {name} for price change...")
            
            model.fit(X_train_scaled, y_train_change)
            
            train_pred = model.predict(X_train_scaled)
            val_pred = model.predict(X_val_scaled)
            test_pred = model.predict(X_test_scaled)
            
            metrics = {
                'train': {
                    'mae': mean_absolute_error(y_train_change, train_pred),
                    'rmse': np.sqrt(mean_squared_error(y_train_change, train_pred)),
                    'r2': r2_score(y_train_change, train_pred)
                },
                'val': {
                    'mae': mean_absolute_error(y_val_change, val_pred),
                    'rmse': np.sqrt(mean_squared_error(y_val_change, val_pred)),
                    'r2': r2_score(y_val_change, val_pred)
                },
                'test': {
                    'mae': mean_absolute_error(y_test_change, test_pred),
                    'rmse': np.sqrt(mean_squared_error(y_test_change, test_pred)),
                    'r2': r2_score(y_test_change, test_pred)
                }
            }
            
            overfitting_score = abs(metrics['train']['r2'] - metrics['val']['r2'])
            
            print(f"    üìä Results:")
            print(f"      Train R¬≤: {metrics['train']['r2']:.4f}")
            print(f"      Val R¬≤:   {metrics['val']['r2']:.4f}")
            print(f"      Test R¬≤:  {metrics['test']['r2']:.4f}")
            print(f"      Overfitting: {overfitting_score:.4f} {'‚ö†Ô∏è HIGH' if overfitting_score > 0.1 else '‚úÖ OK'}")
            
            change_results[name] = {
                'model': model,
                'metrics': metrics,
                'overfitting_score': overfitting_score
            }
        
        regression_results['change_3h'] = change_results
        
        return regression_results
    
    def train_classification_models(self, datasets: Dict) -> Dict:
        """
        Train classification models for trend prediction
        """
        print("\nüîß TRAINING CLASSIFICATION MODELS")
        print("="*50)
        
        X_train_scaled = self.scalers['regression'].transform(datasets['X_train'])
        X_val_scaled = self.scalers['regression'].transform(datasets['X_val'])
        X_test_scaled = self.scalers['regression'].transform(datasets['X_test'])
        
        classification_results = {}
        
        # Trend prediction (1 hour)
        print("\n1Ô∏è‚É£ Trend Classification (1 hour)")
        y_train = datasets['y_train']['trend_1h']
        y_val = datasets['y_val']['trend_1h']
        y_test = datasets['y_test']['trend_1h']
        
        clf_models = {
            'random_forest': RandomForestClassifier(
                n_estimators=50,
                max_depth=10,
                min_samples_split=10,
                min_samples_leaf=5,
                class_weight='balanced',  # Handle imbalanced data
                random_state=42
            )
        }
        
        trend_results = {}
        for name, model in clf_models.items():
            print(f"\n  üî∏ Training {name}...")
            
            model.fit(X_train_scaled, y_train)
            
            train_pred = model.predict(X_train_scaled)
            val_pred = model.predict(X_val_scaled)
            test_pred = model.predict(X_test_scaled)
            
            metrics = {
                'train': {'accuracy': accuracy_score(y_train, train_pred)},
                'val': {'accuracy': accuracy_score(y_val, val_pred)},
                'test': {'accuracy': accuracy_score(y_test, test_pred)}
            }
            
            overfitting_score = abs(metrics['train']['accuracy'] - metrics['val']['accuracy'])
            
            print(f"    üìä Results:")
            print(f"      Train Acc: {metrics['train']['accuracy']:.4f}")
            print(f"      Val Acc:   {metrics['val']['accuracy']:.4f}")
            print(f"      Test Acc:  {metrics['test']['accuracy']:.4f}")
            print(f"      Overfitting: {overfitting_score:.4f} {'‚ö†Ô∏è HIGH' if overfitting_score > 0.1 else '‚úÖ OK'}")
            
            trend_results[name] = {
                'model': model,
                'metrics': metrics,
                'overfitting_score': overfitting_score
            }
        
        classification_results['trend_1h'] = trend_results
        
        return classification_results
    
    def select_best_models(self, regression_results: Dict, classification_results: Dict) -> Dict:
        """
        Ch·ªçn model t·ªët nh·∫•t d·ª±a tr√™n validation performance v√† overfitting
        """
        print("\nüèÜ SELECTING BEST MODELS")
        print("="*50)
        
        best_models = {}
        
        # Best price prediction model
        best_price_model = None
        best_price_score = -float('inf')
        
        for model_name, results in regression_results['price_1h'].items():
            # Scoring: Val R2 - overfitting penalty
            score = results['metrics']['val']['r2'] - (results['overfitting_score'] * 0.5)
            print(f"  {model_name}: Val R¬≤ {results['metrics']['val']['r2']:.4f}, Score: {score:.4f}")
            
            if score > best_price_score:
                best_price_score = score
                best_price_model = (model_name, results)
        
        best_models['price_prediction'] = best_price_model
        print(f"  ü•á Best Price Model: {best_price_model[0]} (Score: {best_price_score:.4f})")
        
        # Best change prediction model
        best_change_model = None
        best_change_score = -float('inf')
        
        for model_name, results in regression_results['change_3h'].items():
            score = results['metrics']['val']['r2'] - (results['overfitting_score'] * 0.5)
            print(f"  {model_name}: Val R¬≤ {results['metrics']['val']['r2']:.4f}, Score: {score:.4f}")
            
            if score > best_change_score:
                best_change_score = score
                best_change_model = (model_name, results)
        
        best_models['change_prediction'] = best_change_model
        print(f"  ü•á Best Change Model: {best_change_model[0]} (Score: {best_change_score:.4f})")
        
        # Best trend prediction model
        best_trend_model = None
        best_trend_score = -float('inf')
        
        for model_name, results in classification_results['trend_1h'].items():
            score = results['metrics']['val']['accuracy'] - (results['overfitting_score'] * 0.5)
            print(f"  {model_name}: Val Acc {results['metrics']['val']['accuracy']:.4f}, Score: {score:.4f}")
            
            if score > best_trend_score:
                best_trend_score = score
                best_trend_model = (model_name, results)
        
        best_models['trend_prediction'] = best_trend_model
        print(f"  ü•á Best Trend Model: {best_trend_model[0]} (Score: {best_trend_score:.4f})")
        
        return best_models
    
    def save_models(self, best_models: Dict, datasets: Dict) -> None:
        """
        L∆∞u c√°c model t·ªët nh·∫•t
        """
        print("\nüíæ SAVING BEST MODELS")
        print("="*50)
        
        root = project_root_path()
        models_dir = os.path.join(root, "models")
        os.makedirs(models_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for task, (model_name, results) in best_models.items():
            # Save model
            model = results['model']
            model_filename = f"fixed_{task}_{model_name}_{timestamp}.joblib"
            model_path = os.path.join(models_dir, model_filename)
            joblib.dump(model, model_path)
            
            # Save scaler
            scaler_filename = f"fixed_scaler_{timestamp}.joblib"
            scaler_path = os.path.join(models_dir, scaler_filename)
            joblib.dump(self.scalers['regression'], scaler_path)
            
            # Save metadata
            metadata = {
                'model_type': model_name,
                'task': task,
                'features': datasets['feature_names'],
                'metrics': results['metrics'],
                'overfitting_score': results['overfitting_score'],
                'scaler_path': scaler_path,
                'training_date': timestamp,
                'dataset_info': datasets['metadata']
            }
            
            metadata_filename = f"fixed_{task}_{model_name}_{timestamp}_metadata.json"
            metadata_path = os.path.join(models_dir, metadata_filename)
            
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2, default=str)
            
            print(f"  ‚úÖ Saved {task}:")
            print(f"    Model: {model_filename}")
            print(f"    Metadata: {metadata_filename}")
    
    def full_training_pipeline(self, symbols=None, limit_hours=1000):
        """
        Pipeline training ho√†n ch·ªânh
        """
        print("üöÄ FIXED ML TRAINING PIPELINE")
        print("="*60)
        
        # 1. Prepare datasets
        datasets = prepare_ml_datasets_fixed(symbols=symbols, limit_hours=limit_hours)
        
        # 2. Train models
        regression_results = self.train_regression_models(datasets)
        classification_results = self.train_classification_models(datasets)
        
        # 3. Select best models
        best_models = self.select_best_models(regression_results, classification_results)
        
        # 4. Save models
        self.save_models(best_models, datasets)
        
        print("\n‚úÖ TRAINING PIPELINE COMPLETED!")
        print("="*60)
        
        return best_models, datasets

def main():
    """
    Main training function
    """
    try:
        # Initialize trainer
        trainer = FixedMLTrainer()
        
        # Run training with top crypto symbols
        symbols = ['BTC', 'ETH', 'BNB', 'ADA', 'SOL']  # Top 5 symbols
        
        best_models, datasets = trainer.full_training_pipeline(
            symbols=symbols, 
            limit_hours=1500  # More data for better training
        )
        
        print("\nüéâ SUCCESS! Fixed models trained and saved.")
        print("Key improvements:")
        print("  ‚úÖ No data leakage")
        print("  ‚úÖ Proper regularization")
        print("  ‚úÖ Realistic R¬≤ scores")
        print("  ‚úÖ Overfitting detection")
        
    except Exception as e:
        print(f"‚ùå Error in training pipeline: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()