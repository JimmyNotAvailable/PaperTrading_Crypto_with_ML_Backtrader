# ğŸ“‹ Káº¾ HOáº CH TÃI Cáº¤U TRÃšC Dá»° ÃN - KAFKA INTEGRATION

> **Má»¥c tiÃªu**: Chuyá»ƒn Ä‘á»•i tá»« kiáº¿n trÃºc Monolithic sang Event-Driven Microservices vá»›i Apache Kafka
> 
> **PhiÃªn báº£n**: 1.0 - Phase 1 Implementation
> 
> **NgÃ y**: November 28, 2025

---

## ğŸ“Š HIá»†N TRáº NG Dá»° ÃN (AS-IS)

### Cáº¥u trÃºc hiá»‡n táº¡i
```
crypto-ml-trading-project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ bot.py                    # Discord bot (monolithic)
â”‚   â”œâ”€â”€ ml/                       # ML algorithms
â”‚   â”‚   â”œâ”€â”€ algorithms/          # LinearRegression, KNN, KMeans
â”‚   â”‚   â”œâ”€â”€ train_all.py         # Training orchestrator
â”‚   â”‚   â””â”€â”€ model_registry.py    # Model versioning
â”‚   â”œâ”€â”€ data_collector/          # Binance API scraper
â”‚   â”‚   â””â”€â”€ realtime_collector.py
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ trainer.py           # Auto training service
â”‚       â””â”€â”€ store.py             # File-based storage
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ realtime/                # CSV/JSON data files
â”‚   â””â”€â”€ models_production/       # .joblib/.pkl files
â”œâ”€â”€ models/                       # Training artifacts
â””â”€â”€ docs/                         # Documentation
```

### Váº¥n Ä‘á» cáº§n giáº£i quyáº¿t

1. **Tight Coupling**: Bot trá»±c tiáº¿p gá»i ML models vÃ  Binance API
2. **File-based Storage**: Dá»¯ liá»‡u lÆ°u CSV/JSON, khÃ³ scale vÃ  sync
3. **No Message Queue**: KhÃ´ng cÃ³ buffer khi API Binance cháº­m hoáº·c ML tÃ­nh toÃ¡n lÃ¢u
4. **Single Point of Failure**: Bot crash = toÃ n bá»™ há»‡ thá»‘ng ngÆ°ng
5. **Limited Algorithms**: Chá»‰ cÃ³ 3 thuáº­t toÃ¡n cÅ© (Linear, KNN, KMeans)

---

## ğŸ¯ Má»¤C TIÃŠU TÃI Cáº¤U TRÃšC (TO-BE)

### Kiáº¿n trÃºc má»›i (Kafka-based Microservices)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     APACHE KAFKA CLUSTER                         â”‚
â”‚                                                                  â”‚
â”‚  Topics:                                                         â”‚
â”‚  â€¢ crypto.market_data    â†’ OHLCV tá»« Binance                     â”‚
â”‚  â€¢ crypto.ml_signals     â†’ Predictions tá»« ML Service            â”‚
â”‚  â€¢ crypto.orders         â†’ Trading decisions                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                    â–²                    â–²
         â”‚                    â”‚                    â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚ Producerâ”‚         â”‚ML Serviceâ”‚        â”‚Decision â”‚
    â”‚ Service â”‚         â”‚(Consumer)â”‚        â”‚ Engine  â”‚
    â”‚         â”‚         â”‚          â”‚        â”‚         â”‚
    â”‚ Binance â”‚         â”‚RandomFor-â”‚        â”‚Backtra- â”‚
    â”‚   API   â”‚         â”‚est + SVM â”‚        â”‚  der    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚+ Logisticâ”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                   â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Discord  â”‚        â”‚Streamlit â”‚
              â”‚   Bot    â”‚        â”‚Dashboard â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Lá»£i Ã­ch

âœ… **Decoupling**: Má»—i service Ä‘á»™c láº­p, dá»… maintain
âœ… **Scalability**: CÃ³ thá»ƒ cháº¡y nhiá»u ML Consumer song song
âœ… **Resilience**: Kafka buffer data khi service táº¡m ngÆ°ng
âœ… **Real Big Data**: Sáºµn sÃ ng cho millions records/day
âœ… **Modern Stack**: PhÃ¹ há»£p CDIO & Production-ready

---

## ğŸ“ Cáº¤U TRÃšC THÆ¯ Má»¤C Má»šI (Äá» xuáº¥t)

```
crypto-ml-trading-project/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ copilot-instructions.md          # â† ÄÃƒ Táº O
â”‚
â”œâ”€â”€ docker-compose.yml                   # â† Cáº¦N Cáº¬P NHáº¬T (Kafka + Zookeeper + UI)
â”œâ”€â”€ .env.example                         # â† Template cho biáº¿n mÃ´i trÆ°á»ng
â”œâ”€â”€ .env                                 # â† Secrets (KHÃ”NG commit)
â”œâ”€â”€ .gitignore                           # â† ÄÃ£ cÃ³ (kiá»ƒm tra .env, data/)
â”œâ”€â”€ requirements.txt                     # â† Cáº¦N Bá»” SUNG confluent-kafka, backtrader
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ producers/                       # â† Má»šI: Kafka Producers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ binance_producer.py         # Thu tháº­p data â†’ Kafka
â”‚   â”‚   â””â”€â”€ config.py                   # Kafka connection config
â”‚   â”‚
â”‚   â”œâ”€â”€ consumers/                       # â† Má»šI: Kafka Consumers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ml_consumer.py              # Consume market_data â†’ predict â†’ produce signals
â”‚   â”‚   â””â”€â”€ decision_consumer.py        # Consume signals â†’ Backtrader â†’ produce orders
â”‚   â”‚
â”‚   â”œâ”€â”€ ml/                              # â† GIá»® NGUYÃŠN + NÃ‚NG Cáº¤P
â”‚   â”‚   â”œâ”€â”€ algorithms/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py                 # Abstract BaseModel
â”‚   â”‚   â”‚   â”œâ”€â”€ linear_regression.py    # â† Giá»¯ láº¡i
â”‚   â”‚   â”‚   â”œâ”€â”€ knn_models.py           # â† Giá»¯ láº¡i
â”‚   â”‚   â”‚   â”œâ”€â”€ clustering.py           # â† Giá»¯ láº¡i (K-Means)
â”‚   â”‚   â”‚   â”œâ”€â”€ random_forest.py        # â† ÄÃƒ CÃ“ - Cáº¦N KIá»‚M TRA
â”‚   â”‚   â”‚   â”œâ”€â”€ svm_models.py           # â† Má»šI (Support Vector Machine)
â”‚   â”‚   â”‚   â””â”€â”€ logistic_regression.py  # â† ÄÃƒ CÃ“ - Cáº¦N KIá»‚M TRA
â”‚   â”‚   â”œâ”€â”€ core.py
â”‚   â”‚   â”œâ”€â”€ data_prep.py
â”‚   â”‚   â”œâ”€â”€ train_all.py                # â† Cáº¬P NHáº¬T: train 3 models má»›i
â”‚   â”‚   â””â”€â”€ model_registry.py
â”‚   â”‚
â”‚   â”œâ”€â”€ backtrader/                      # â† Má»šI: Decision Engine
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ strategies/
â”‚   â”‚   â”‚   â”œâ”€â”€ ml_strategy.py          # Strategy nháº­n signals tá»« Kafka
â”‚   â”‚   â”‚   â””â”€â”€ risk_management.py      # Stop loss, take profit logic
â”‚   â”‚   â””â”€â”€ virtual_exchange.py         # SÃ n áº£o Ä‘á»ƒ demo
â”‚   â”‚
â”‚   â”œâ”€â”€ bot.py                           # â† GIá»® NGUYÃŠN nhÆ°ng refactor
â”‚   â”‚                                    #    Sáº½ consume tá»« Kafka thay vÃ¬ gá»i trá»±c tiáº¿p
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ trainer.py                  # â† Giá»¯ nguyÃªn
â”‚   â”‚   â””â”€â”€ store.py                    # â† Giá»¯ nguyÃªn (chuáº©n bá»‹ MongoDB)
â”‚   â”‚
â”‚   â””â”€â”€ utils/                           # â† Má»šI: Shared utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py                   # Centralized logging
â”‚       â””â”€â”€ config_loader.py            # Load .env safely
â”‚
â”œâ”€â”€ config/                              # â† Má»šI: Configuration files
â”‚   â”œâ”€â”€ kafka_config.py
â”‚   â”œâ”€â”€ mongodb_config.py               # â† ÄÃƒ CÃ“
â”‚   â””â”€â”€ production_config.py            # â† ÄÃƒ CÃ“
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ realtime/                       # â† Giá»¯ nguyÃªn (legacy)
â”‚   â”œâ”€â”€ cache/                          # â† Giá»¯ nguyÃªn
â”‚   â””â”€â”€ models_production/              # â† Giá»¯ nguyÃªn
â”‚
â”œâ”€â”€ models/                              # â† Giá»¯ nguyÃªn (.joblib files)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_environment.ps1           # â† ÄÃƒ CÃ“
â”‚   â””â”€â”€ init_kafka_topics.py            # â† Má»šI: Táº¡o topics tá»± Ä‘á»™ng
â”‚
â”œâ”€â”€ web/                                 # â† Má»šI: Streamlit Dashboard
â”‚   â”œâ”€â”€ app.py                          # Main Streamlit app
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ price_chart.py              # Real-time chart
â”‚   â”‚   â”œâ”€â”€ ml_predictions.py           # ML signals display
â”‚   â”‚   â””â”€â”€ virtual_portfolio.py        # Portfolio tracker
â”‚   â””â”€â”€ requirements.txt                # streamlit, plotly
â”‚
â”œâ”€â”€ tests/                               # â† Má» Rá»˜NG
â”‚   â”œâ”€â”€ test_producers.py
â”‚   â”œâ”€â”€ test_consumers.py
â”‚   â””â”€â”€ test_ml_models.py
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ RESTRUCTURING_PLAN_KAFKA.md     # â† FILE NÃ€Y
    â”œâ”€â”€ Step_1.md                       # â† ÄÃƒ CÃ“ (Phase 1 guide)
    â”œâ”€â”€ ToturialUpgrade.md              # â† ÄÃƒ CÃ“ (Upgrade guide)
    â”œâ”€â”€ DanhGiaTongQuan.md              # â† ÄÃƒ CÃ“ (Overall assessment)
    â””â”€â”€ KAFKA_TOPICS_SCHEMA.md          # â† Má»šI: Kafka message schemas
```

---

## ğŸ”§ PHASE 1: SETUP INFRASTRUCTURE (Tuáº§n 1-2)

### BÆ°á»›c 1.1: Cáº­p nháº­t Dependencies

**File: `requirements.txt`**
```txt
# Existing
pymongo
requests
scikit-learn
numpy
pandas
discord.py
python-dotenv
joblib
psutil

# â† THÃŠM Má»šI cho Kafka
confluent-kafka==2.3.0         # Kafka Python client (C-based, nhanh)

# â† THÃŠM Má»šI cho Backtrader
backtrader==1.9.78.123         # Trading framework

# â† THÃŠM Má»šI cho Dashboard
streamlit==1.28.0
plotly==5.18.0

# â† THÃŠM Má»šI cho Testing
pytest==7.4.3
pytest-asyncio==0.21.1
```

**CÃ i Ä‘áº·t**:
```powershell
.\crypto-venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

### BÆ°á»›c 1.2: Setup Kafka vá»›i Docker Compose

**File: `docker-compose.yml` (Cáº¬P NHáº¬T)**

```yaml
version: '3.8'

services:
  # ========== KAFKA INFRASTRUCTURE ==========
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    hostname: zookeeper
    container_name: crypto_zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    hostname: kafka
    container_name: crypto_kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"      # External access (localhost Python)
      - "29092:29092"    # Internal access (Docker network)
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 5

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: crypto_kafka_ui
    ports:
      - "8080:8080"
    depends_on:
      - kafka
    environment:
      KAFKA_CLUSTERS_0_NAME: crypto_cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181

  # ========== MONGODB (Existing) ==========
  mongo:
    image: mongo:latest
    container_name: crypto_mongo
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
      - ./db/init:/docker-entrypoint-initdb.d

  # ========== PYTHON SERVICES (To be added later) ==========
  # producer:
  #   build: .
  #   container_name: crypto_producer
  #   environment:
  #     - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
  #   depends_on:
  #     - kafka

volumes:
  zookeeper_data:
  zookeeper_logs:
  kafka_data:
  mongo_data:
```

**Khá»Ÿi Ä‘á»™ng**:
```powershell
docker-compose up -d
docker ps  # Kiá»ƒm tra 4 containers: zookeeper, kafka, kafka-ui, mongo
```

**Kiá»ƒm tra Kafka UI**: Má»Ÿ http://localhost:8080

### BÆ°á»›c 1.3: Táº¡o Kafka Topics

**File: `scripts/init_kafka_topics.py` (Má»šI)**

```python
#!/usr/bin/env python3
"""
Khá»Ÿi táº¡o Kafka topics cho Crypto ML Trading Project
"""

from confluent_kafka.admin import AdminClient, NewTopic
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_topics():
    """Táº¡o cÃ¡c Kafka topics cáº§n thiáº¿t"""
    
    admin_client = AdminClient({
        'bootstrap.servers': 'localhost:9092'
    })
    
    topics = [
        NewTopic(
            topic='crypto.market_data',
            num_partitions=3,
            replication_factor=1,
            config={
                'retention.ms': '86400000',  # 24 hours
                'compression.type': 'gzip'
            }
        ),
        NewTopic(
            topic='crypto.ml_signals',
            num_partitions=3,
            replication_factor=1,
            config={
                'retention.ms': '604800000',  # 7 days
                'compression.type': 'gzip'
            }
        ),
        NewTopic(
            topic='crypto.orders',
            num_partitions=1,
            replication_factor=1,
            config={
                'retention.ms': '2592000000',  # 30 days
                'cleanup.policy': 'compact'
            }
        )
    ]
    
    # Táº¡o topics
    fs = admin_client.create_topics(topics)
    
    for topic, f in fs.items():
        try:
            f.result()
            logger.info(f"âœ… Topic '{topic}' created successfully")
        except Exception as e:
            logger.error(f"âŒ Failed to create topic '{topic}': {e}")

if __name__ == "__main__":
    create_topics()
```

**Cháº¡y**:
```powershell
python scripts\init_kafka_topics.py
```

### BÆ°á»›c 1.4: Cáº­p nháº­t .env Template

**File: `.env.example`**

```env
# ========== DISCORD BOT ==========
DISCORD_BOT_TOKEN=your-discord-token-here

# ========== KAFKA CONFIGURATION ==========
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_GROUP_ID=crypto_ml_group

# ========== BINANCE API ==========
BINANCE_API_KEY=your-binance-api-key
BINANCE_SECRET_KEY=your-binance-secret-key

# ========== MONGODB ==========
MONGODB_URI=mongodb://localhost:27017/crypto

# ========== CURRENCY ==========
FX_USD_VND=24000

# ========== ML SETTINGS ==========
ML_MODEL_PATH=./models
ML_PREDICTION_THRESHOLD=0.7

# ========== LOGGING ==========
LOG_LEVEL=INFO
```

**Copy Ä‘á»ƒ sá»­ dá»¥ng**:
```powershell
copy .env.example .env
# Sau Ä‘Ã³ chá»‰nh sá»­a .env vá»›i thÃ´ng tin thá»±c táº¿
```

---

## ğŸ“ PHASE 2: IMPLEMENT PRODUCERS (Tuáº§n 3)

### File: `app/producers/binance_producer.py` (Má»šI)

```python
#!/usr/bin/env python3
"""
Kafka Producer: Thu tháº­p dá»¯ liá»‡u crypto tá»« Binance â†’ Kafka topic
"""

import os
import json
import time
from datetime import datetime
from typing import Dict, List
import logging

from confluent_kafka import Producer
import requests
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BinanceKafkaProducer:
    """Producer service: Binance API â†’ Kafka"""
    
    def __init__(self):
        self.kafka_config = {
            'bootstrap.servers': os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092'),
            'client.id': 'binance_producer'
        }
        self.producer = Producer(self.kafka_config)
        self.topic = 'crypto.market_data'
        
        self.binance_api = "https://api.binance.com/api/v3"
        self.symbols = [
            'BTCUSDT', 'ETHUSDT', 'ADAUSDT', 'DOGEUSDT',
            'XRPUSDT', 'DOTUSDT', 'LTCUSDT', 'LINKUSDT'
        ]
    
    def delivery_report(self, err, msg):
        """Callback khi message Ä‘Æ°á»£c gá»­i"""
        if err is not None:
            logger.error(f'âŒ Message delivery failed: {err}')
        else:
            logger.debug(f'âœ… Message delivered to {msg.topic()} [{msg.partition()}]')
    
    def fetch_market_data(self, symbol: str) -> Dict:
        """Láº¥y dá»¯ liá»‡u OHLCV tá»« Binance"""
        try:
            # Current price
            ticker_url = f"{self.binance_api}/ticker/24hr?symbol={symbol}"
            response = requests.get(ticker_url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                market_data = {
                    'symbol': symbol,
                    'timestamp': int(time.time() * 1000),
                    'price': float(data['lastPrice']),
                    'open': float(data['openPrice']),
                    'high': float(data['highPrice']),
                    'low': float(data['lowPrice']),
                    'volume': float(data['volume']),
                    'price_change_pct': float(data['priceChangePercent'])
                }
                
                return market_data
            else:
                logger.error(f"Binance API error: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"Error fetching data for {symbol}: {e}")
            return None
    
    def produce_market_data(self):
        """Main loop: Thu tháº­p vÃ  gá»­i dá»¯ liá»‡u vÃ o Kafka"""
        logger.info(f"ğŸš€ Starting Binance Producer...")
        logger.info(f"ğŸ“¡ Kafka: {self.kafka_config['bootstrap.servers']}")
        logger.info(f"ğŸ“Š Symbols: {', '.join(self.symbols)}")
        
        while True:
            try:
                for symbol in self.symbols:
                    data = self.fetch_market_data(symbol)
                    
                    if data:
                        # Serialize to JSON
                        message = json.dumps(data).encode('utf-8')
                        
                        # Send to Kafka
                        self.producer.produce(
                            self.topic,
                            key=symbol.encode('utf-8'),
                            value=message,
                            callback=self.delivery_report
                        )
                        
                        logger.info(f"ğŸ“¤ Produced: {symbol} @ ${data['price']:,.2f}")
                
                # Flush buffer
                self.producer.flush()
                
                # Wait 60s before next batch
                time.sleep(60)
                
            except KeyboardInterrupt:
                logger.info("â¹ï¸ Producer stopped by user")
                break
            except Exception as e:
                logger.error(f"âŒ Error in producer loop: {e}")
                time.sleep(10)

if __name__ == "__main__":
    producer = BinanceKafkaProducer()
    producer.produce_market_data()
```

**Cháº¡y Producer**:
```powershell
python app\producers\binance_producer.py
```

---

## ğŸ“Š PHASE 3: ML MODELS UPGRADE (Tuáº§n 4-5)

### BÆ°á»›c 3.1: Implement Random Forest (Primary Model)

**File: `app/ml/algorithms/random_forest.py`** (ÄÃƒ Tá»’N Táº I - kiá»ƒm tra)

```python
# Kiá»ƒm tra file nÃ y Ä‘Ã£ cÃ³ chÆ°a:
ls app\ml\algorithms\random_forest.py

# Náº¿u chÆ°a cÃ³, cáº§n táº¡o má»›i theo pattern BaseModel
```

### BÆ°á»›c 3.2: Implement SVM

**File: `app/ml/algorithms/svm_models.py` (Má»šI)**

```python
from sklearn.svm import SVR, SVC
from sklearn.preprocessing import StandardScaler
from .base import BaseModel
# ... (implement theo pattern LinearRegressionModel)
```

### BÆ°á»›c 3.3: Verify Logistic Regression

```python
# Kiá»ƒm tra file:
ls app\ml\algorithms\logistic_regression.py
# â†’ ÄÃƒ Tá»’N Táº I â†’ Cáº§n kiá»ƒm tra tuÃ¢n thá»§ BaseModel
```

### BÆ°á»›c 3.4: Update train_all.py

ThÃªm 3 models má»›i vÃ o training pipeline:
- Random Forest (primary)
- SVM
- Logistic Regression

---

## ğŸ¤– PHASE 4: ML CONSUMER SERVICE (Tuáº§n 6)

**File: `app/consumers/ml_consumer.py` (Má»šI)**

```python
#!/usr/bin/env python3
"""
Kafka Consumer: Nháº­n market_data â†’ ML Prediction â†’ Produce signals
"""

import os
import json
import joblib
from confluent_kafka import Consumer, Producer
# ... (consume tá»« crypto.market_data, predict, produce vÃ o crypto.ml_signals)
```

---

## ğŸ“ˆ PHASE 5: BACKTRADER INTEGRATION (Tuáº§n 7-8)

**File: `app/backtrader/strategies/ml_strategy.py` (Má»šI)**

```python
import backtrader as bt

class MLSignalStrategy(bt.Strategy):
    """Strategy nháº­n signals tá»« Kafka topic crypto.ml_signals"""
    # ... (implement logic mua/bÃ¡n dá»±a trÃªn ML predictions)
```

---

## ğŸ¨ PHASE 6: STREAMLIT DASHBOARD (Tuáº§n 9)

**File: `web/app.py` (Má»šI)**

```python
import streamlit as st
from confluent_kafka import Consumer
# ... (3-column layout: Price Chart | ML Signals | Virtual Portfolio)
```

---

## âœ… CHECKLIST TRIá»‚N KHAI

### Phase 1: Infrastructure âœ…
- [ ] Cáº­p nháº­t `requirements.txt` vá»›i confluent-kafka, backtrader
- [ ] Cáº­p nháº­t `docker-compose.yml` vá»›i Kafka/Zookeeper/UI
- [ ] Cháº¡y `docker-compose up -d`
- [ ] Cháº¡y `scripts/init_kafka_topics.py`
- [ ] Kiá»ƒm tra Kafka UI (localhost:8080)
- [ ] Táº¡o `.env` tá»« `.env.example`

### Phase 2: Producer âœ…
- [ ] Táº¡o `app/producers/binance_producer.py`
- [ ] Test producer: `python app\producers\binance_producer.py`
- [ ] Verify messages trong Kafka UI

### Phase 3: ML Upgrade
- [ ] Kiá»ƒm tra `random_forest.py` (Ä‘Ã£ cÃ³ sáºµn?)
- [ ] Táº¡o `svm_models.py`
- [ ] Verify `logistic_regression.py`
- [ ] Update `train_all.py` Ä‘á»ƒ train 3 models
- [ ] Cháº¡y training: `python app\ml\train_all.py`

### Phase 4: Consumer
- [ ] Táº¡o `app/consumers/ml_consumer.py`
- [ ] Test consumer vá»›i producer
- [ ] Verify predictions trong topic `crypto.ml_signals`

### Phase 5: Backtrader
- [ ] CÃ i `pip install backtrader`
- [ ] Táº¡o `app/backtrader/strategies/ml_strategy.py`
- [ ] Táº¡o `virtual_exchange.py` cho demo

### Phase 6: Dashboard
- [ ] Táº¡o `web/app.py`
- [ ] Implement real-time chart vá»›i Plotly
- [ ] Test: `streamlit run web\app.py`

---

## ğŸš¨ Rá»¦I RO & GIáº¢I PHÃP

| Rá»§i ro | Giáº£i phÃ¡p |
|---------|-----------|
| **Kafka quÃ¡ náº·ng cho dev** | Chá»‰ cháº¡y khi test, táº¯t khi khÃ´ng dÃ¹ng: `docker-compose down` |
| **Data leakage trong ML** | Báº¯t buá»™c `TimeSeriesSplit`, review `data_prep.py` |
| **Latency >500ms** | Profile code, optimize feature engineering |
| **Token leak** | RÃ  soÃ¡t `.gitignore`, khÃ´ng commit `.env` |

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

- `Step_1.md` - HÆ°á»›ng dáº«n Phase 1 chi tiáº¿t
- `ToturialUpgrade.md` - Kafka architecture diagram
- `DanhGiaTongQuan.md` - ÄÃ¡nh giÃ¡ ML algorithms
- `.github/copilot-instructions.md` - AI agent guide

---

**Author**: AI Restructuring Plan
**Date**: November 28, 2025
**Version**: 1.0
